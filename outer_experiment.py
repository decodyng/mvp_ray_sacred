from inner_experiment import inner_ex
from sacred import Experiment
from sacred.observers import FileStorageObserver
from ray import tune
import os.path as osp
import ray
from utils import sacred_copy, update

outer_exp = Experiment('outer_exp', ingredients=[inner_ex])


def worker_function(inner_ex_config, config):
    """
    Combines experiment config and auto-generated Ray config, and runs an iteration of
    inner_ex on that combined config.

    :param inner_ex_config: The current values of inner experiment config, including
    any modifications we might have made in an macro_experiment config update
    :param config: Config generated by Ray tune
    :return:
    """
    from inner_experiment import inner_ex
    # Something that runs inner_ex by combining "base" config and ray experiment config
    inner_ex_dict = dict(inner_ex_config)
    merged_config = update(inner_ex_dict, config)

    observer = FileStorageObserver.create(osp.join('inner_nested_results'))
    inner_ex.observers.append(observer)
    ret_val = inner_ex.run(config_updates=merged_config)
    ray.tune.track.log(accuracy=ret_val.result)


@outer_exp.config
def base_config():
    spec = {}
    ray_server = None # keeping this here as a reminder we could start an autoscaling server if we wanted


@outer_exp.named_config
def hyperparameter_search(inner_ex):
    """
    :param inner_ex: The config dict for inner_ex, available because it is an ingredient of macro_ex
    :return:
    """
    exp_name = "hyperparameter_search"
    spec = {"exponent": tune.grid_search(list(range(1, 50)))}
    modified_inner_ex = dict(inner_ex)
    # To test that we can run tuning jobs with parameters modified from default config
    # but not being sampled over through Ray
    modified_inner_ex['offset'] = 8


@outer_exp.main
def multi_main(modified_inner_ex, exp_name, spec):
    inner_ex_config = sacred_copy(modified_inner_ex)

    def trainable_function(config):
        # Turns out functools.partial() doesn't parse as a function for Ray's purpose of
        # validating the passed-in run function, so I'm doing this silly thing
        worker_function(inner_ex_config, config)

    # Need to sacred_copy spec because otherwise it's a ReadOnlyDict, which causes problems
    spec_copy = sacred_copy(spec)
    analysis = tune.run(
        trainable_function,
        name=exp_name,
        config=spec_copy
    )
    best_config = analysis.get_best_config(metric="accuracy")
    print(f"Best config is: {best_config}")
    print("Results available at: ")
    print(analysis._get_trial_paths())


def main():
    observer = FileStorageObserver.create('macro_results')
    outer_exp.observers.append(observer)
    outer_exp.run_commandline()


if __name__ == '__main__':
    main()
